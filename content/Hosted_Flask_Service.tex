\chapter{Self-hosted Flask Service}
\label{cha:hosted_flask_service}
\textbf{Author:} Luna Schätzle

\textbf{Author:} Florian Prandstetter (docker section \ref{sec:docker})

This chapter details the implementation, architecture, and deployment of a self-hosted Flask service that serves as a critical 
interface linking the front-end and back-end components of the system. It outlines the technical design and deployment strategies 
while critically evaluating the benefits of a self-hosted solution and the rationale behind key architectural decisions. The service, 
which underpins both the Student AI Hub and the code extensions backend, was developed to address specific operational requirements. 
This section provides a comprehensive overview of the motivations for its development, elaborates on its core functionalities, 
and situates its role within the broader system architecture—thus laying the groundwork for the ensuing technical discussion.


\section{Advantages of a Self-hosted Service}
A self-hosted service confers a multitude of benefits relative to externally managed or third-party solutions. This section examines these advantages in depth:
\begin{itemize}
    \item \textbf{Enhanced Customization and Environmental Control:} By hosting the service internally, developers gain complete authority over the configuration and optimization of the operating environment. This control facilitates the implementation of domain-specific modifications and enables precise tuning to meet the unique needs of the project.
    \item \textbf{Rapid Prototyping and Agile Deployment:} The self-hosted nature of the service supports agile development practices. New features and bespoke functionalities can be rapidly prototyped, iteratively tested, and deployed, thereby significantly reducing development cycles and accelerating time-to-market.
    \item \textbf{Improved Data Security and Regulatory Compliance:} Hosting the service in-house allows for stringent oversight of data management practices. This approach is particularly advantageous in contexts governed by strict data protection regulations and institutional policies, as it enables the implementation of tailored security measures and enhances overall control over sensitive information.
\end{itemize}
Collectively, these factors validate the strategic decision to pursue a self-hosted approach, underscoring its technical, operational, and regulatory merits.

\section{Flask as a Web Framework}

Flask is a lightweight, yet versatile web Python framework that plays a central role in the development of our Student AI Hub. Its minimalistic design and powerful capabilities make it an excellent choice for both rapid prototyping and scalable application development.

\subsection{Core Functionalities of Flask}

Flask's architecture is built around several core functionalities that streamline web application development:
\begin{itemize}
    \item \textbf{Request Routing:} Flask's routing system allows for the efficient mapping of URLs to Python functions. This ensures that incoming web requests are correctly handled and directed to the appropriate endpoints, which is crucial for managing user interactions.
    \item \textbf{Middleware Support:} Flask supports middleware, facilitating the insertion of additional processing layers before or after a request is handled. This capability is essential for tasks such as authentication, logging, and error handling.
    \item \textbf{Extensibility:} Its modular design allows developers to integrate third-party extensions or develop custom modules, thereby expanding Flask's functionalities to meet the evolving demands of the project.
    \item \textbf{Simplicity and Clarity:} Flask's clear and concise API encourages clean, maintainable code. This simplicity does not come at the expense of power; rather, it provides a robust foundation upon which complex systems can be built.
\end{itemize}

These features collectively contribute to an efficient workflow, ensuring that web requests and responses are managed in a structured and scalable manner.

\subsection{Rationale for Selecting Flask}

The decision to adopt Flask as the backbone of our web service was influenced by several key considerations:
\begin{itemize}
    \item \textbf{Simplicity and Flexibility:} Flask's minimalistic core allows for rapid development and iterative prototyping without the overhead of a full-stack framework. This flexibility is invaluable during the early stages of project development and testing.
    \item \textbf{Extensive Documentation and Community Support:} With comprehensive documentation and a vibrant community, Flask offers abundant resources for troubleshooting and extending functionality. This support network accelerates development and helps resolve challenges efficiently.
    \item \textbf{Scalability:} Despite its simplicity, Flask is designed to scale. Its modular nature means that as the project grows, new features can be seamlessly integrated, ensuring that the framework remains robust even under increased demand.
    \item \textbf{Integration Capabilities:} Flask can be easily combined with various libraries and APIs. This makes it particularly well-suited for integrating AI models, data processing tools, and other external services—core requirements of the Student AI Hub.
\end{itemize}

In summary, Flask's combination of ease-of-use, extensibility, and strong community support provides a solid and adaptable framework for developing a dynamic web service. Its ability to handle everything from simple routing to complex middleware interactions makes it the ideal choice for building a robust backend that meets the diverse needs of our project.

\section{Architecture and Service Structure}

The Flask service functions as a robust and flexible backend for the Student AI Hub and the code extension. Its modular and extensible design facilitates the seamless integration of additional functionalities and services. 

\subsection{System Architecture}

The figure below illustrates the key architectural components of the Flask service and their interactions.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/flask_service.png}
    \caption{Flask Service Architecture}
    \label{fig:flask_service_architecture}
\end{figure}

The system is organized around a central Python script, \texttt{app.py}, which handles incoming requests, registers endpoints, initiates the server, and manages logging. Server configuration is maintained in \texttt{config.py}, where settings such as the language model, image processing rules, upload folder, and conversion language are defined.

Endpoints are organized within the \texttt{routes} subfolder, with similar endpoints grouped together. They are divided into the following files:
\begin{itemize}
    \item \textbf{chatbot\_route.py:} Contains endpoints related to chatbot functionality.
    \item \textbf{OCR\_route.py:} Manages endpoints for image to text conversion.
    \item \textbf{Programming\_bot\_route.py:} Houses endpoints for the programming bot.
\end{itemize}


Within these files, endpoints process user requests and return responses, and are designed for easy extension. They also handle error management and user feedback by converting data from the user and the Ollama API into the appropriate format when necessary. Connections to the Ollama API and the OCR (Optical Character Recognition) system are managed within the \texttt{Utils} folder, which contains all utility functions used by the endpoints, including those for sending requests, performing OCR, and converting Markdown to HTML.

\subsection{Expandability and Modularity}

The server is designed for easy expandability and modularity. Endpoints are stored in separate files, which allows for the straightforward addition of new functionality. Similarly, utility functions can be updated or replaced with minimal effort, and the server configuration is flexible enough to accommodate new parameters.

This modular and extensible architecture ensures that the Flask service can readily adapt to future enhancements and integrations. These design principles contribute to a flexible, maintainable server that can be effortlessly updated with new features and functionalities.

\section{RESTful Endpoints and Functionalities}
\label{sec:endpoints}

This section provides a comprehensive overview of the RESTful endpoints implemented in the Flask service. Each endpoint is detailed in terms of its input parameters, expected output, and the underlying logic that processes incoming requests.

\subsection{Chatbot Endpoints}

The Chatbot endpoints are responsible for handling all server requests associated with both the general chatbot and the image recognition chatbot.

\paragraph{Chatbot Endpoint}
The following code snippet illustrates the endpoint for the general chatbot:

\begin{lstlisting}[language=Python, caption={Chatbot Endpoint}]
@chat_bp.route('/ask_ollama', methods=['POST'])  # Defines the endpoint
def ask_ollama_endpoint():
    """
    Chat endpoint: Receives a user query and a model, communicates with Ollama, and returns the response.
    """
    data = request.get_json()

    if not data:
        logger.warning('No JSON data provided')
        return jsonify({'error': 'No JSON data provided'}), 400

    prompt = data.get('prompt')
    model = data.get('model', Config.OLLAMA_MODEL_DEFAULT)  # Default model

    if not prompt:
        logger.warning('Prompt missing')
        return jsonify({'error': 'Prompt missing'}), 400

    try:
        response = ask_ollama(prompt, model=model)

        if 'choices' in response:
            return jsonify(response), 200
        else:
            return jsonify(response), 500

    except Exception as e:
        logger.error(f'Error communicating with Ollama: {str(e)}')
        return jsonify({'error': 'Error communicating with Ollama'}), 500
\end{lstlisting}

This endpoint extracts the necessary data from the API request and passes it to the \texttt{ask\_ollama} utility function, which communicates with the Ollama API and returns the corresponding response. If any required data is missing or if the Ollama API is unreachable, the endpoint returns an appropriate error message.

\paragraph{Image Recognition Endpoint}
The following code snippet demonstrates the endpoint for the Image Recognition Chatbot:

\begin{lstlisting}[language=Python, caption={Image Recognition Endpoint}]
@chat_bp.route('/ask_ollama_vision', methods=['POST'])  # Defines the endpoint using POST method
def ask_ollama_vision_endpoint():
    """
    Vision Chat endpoint: Receives a user query with images and a model, communicates with Ollama Vision, and returns the response.
    """
    data = request.get_json()

    if not data:
        logger.warning('No JSON data provided')
        return jsonify({'error': 'No JSON data provided'}), 400

    prompt = data.get('prompt')
    model = data.get('model', 'llama3.2-vision')  # Default model for vision tasks
    images = data.get('images', [])

    if not prompt and not images:
        logger.warning('Neither prompt nor images provided')
        return jsonify({'error': 'Neither prompt nor images provided'}), 400

    # Process the images
    image_paths = []
    try:
        for idx, img_str in enumerate(images):
            # Extract image data from base64 string
            if ',' in img_str:
                header, encoded = img_str.split(',', 1)
            else:
                header, encoded = '', img_str
            img_data = base64.b64decode(encoded)
            img = Image.open(BytesIO(img_data))

            # Validate the image format
            if img.format.lower() not in Config.ALLOWED_EXTENSIONS:
                logger.warning(f'Invalid image format: {img.format}')
                return jsonify({'error': f'Invalid image format: {img.format}'}), 400

            # Optionally reduce or compress image size
            img.thumbnail((1024, 1024))  # Example: maximum size 1024x1024

            # Save the image temporarily
            img_filename = f"temp_{idx}.{img.format.lower()}"
            img_path = os.path.join(Config.UPLOAD_FOLDER, img_filename)
            img.save(img_path)
            image_paths.append(img_path)

    except Exception as e:
        logger.error(f'Error processing images: {str(e)}')
        return jsonify({'error': f'Error processing images: {str(e)}'}), 400

    try:
        response = ask_ollama_vision(prompt, model=model, image_paths=image_paths)

        if 'choices' in response:
            return jsonify(response), 200
        else:
            return jsonify(response), 500

    except Exception as e:
        logger.error(f'Error communicating with Ollama Vision: {str(e)}')
        return jsonify({'error': 'Error communicating with Ollama Vision'}), 500

    finally:
        # Delete temporary images
        for img_path in image_paths:
            if os.path.exists(img_path):
                os.remove(img_path)
\end{lstlisting}

The Image Recognition endpoint functions similarly to the general chatbot endpoint, with the added capability of processing image data. It decodes the base64-encoded images, saves them temporarily, validates their format, and resizes them if necessary to optimize processing speed and reduce server load. Once processed, the images are passed to the \texttt{ask\_ollama\_vision} utility function. After the request is handled, the endpoint ensures that all temporary images are deleted.

Overall, these endpoints are designed for scalability and robustness, efficiently managing both text and image inputs while providing meaningful feedback to the user in case of errors.


\subsection{OCR Endpoints}

This file contains all the OCR endpoints. Currently, there is a single endpoint that handles input for optical character recognition.

The following code snippet illustrates the OCR endpoint:

\begin{lstlisting}[language=Python, caption={OCR Endpoint}]
def allowed_file(filename):
    """
    Checks if the file has an allowed extension.
    """
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in Config.ALLOWED_EXTENSIONS

@ocr_bp.route('/ocr', methods=['POST'])
def process_ocr():
    """
    OCR endpoint: Accepts an image (either as a Base64 string or as a file), extracts text using OCR,
    enhances the extracted text via the Ollama API, and returns both the raw and improved text.
    Supports both JSON and multipart/form-data requests.
    """
    img_path = None  # Initialize img_path for cleanup
    try:
        # Check if the request contains JSON data
        if request.is_json:
            data = request.get_json()
            image_str = data.get('image')
            if not image_str:
                logger.warning('Image data missing in JSON request')
                return jsonify({'error': 'Image data missing'}), 400

            # Decode the Base64 image
            if ',' in image_str:
                header, encoded = image_str.split(',', 1)
            else:
                header, encoded = '', image_str
            try:
                img_data = base64.b64decode(encoded)
                img = Image.open(BytesIO(img_data))
            except Exception as e:
                logger.error(f'Error decoding Base64 image: {str(e)}')
                return jsonify({'error': 'Invalid Base64 image data'}), 400

        else:
            # Handle multipart/form-data request
            if 'image' not in request.files:
                logger.warning('No image file provided in multipart/form-data request')
                return jsonify({'error': 'No image file provided'}), 400

            file = request.files['image']
            if file.filename == '':
                logger.warning('Empty filename in multipart/form-data request')
                return jsonify({'error': 'Empty filename'}), 400

            if file and allowed_file(file.filename):
                try:
                    img = Image.open(file.stream)
                except Exception as e:
                    logger.error(f'Error opening uploaded image file: {str(e)}')
                    return jsonify({'error': 'Invalid image file'}), 400
            else:
                logger.warning(f'Invalid image format: {file.filename}')
                return jsonify({'error': f'Invalid image format. Allowed: {", ".join(Config.ALLOWED_EXTENSIONS)}'}), 400

        # Validate the image format
        if img.format.lower() not in Config.ALLOWED_EXTENSIONS:
            logger.warning(f'Invalid image format: {img.format}')
            return jsonify({'error': f'Invalid image format: {img.format}'}), 400

        # Optionally resize the image to reduce processing time
        img.thumbnail((1024, 1024))  # Example: maximum size 1024x1024

        # Save the image temporarily if required by the OCR tool
        img_filename = "temp_ocr.png"
        img_path = os.path.join(Config.UPLOAD_FOLDER, img_filename)
        try:
            img.save(img_path)
        except Exception as e:
            logger.error(f'Error saving temporary image: {str(e)}')
            return jsonify({'error': 'Error saving image'}), 500

        # Perform OCR using Tesseract
        try:
            extracted_text = pytesseract.image_to_string(img, lang=Config.TESSERACT_LANG)
            if not extracted_text.strip():
                logger.warning('OCR could not extract any text')
                return jsonify({'error': 'OCR could not extract any text'}), 400
        except Exception as e:
            logger.error(f'OCR processing error: {str(e)}')
            return jsonify({'error': 'OCR processing error'}), 500

        # Enhance the extracted text using the Ollama API
        try:
            improved_text = improve_text_with_ollama(extracted_text, model='llama3.2')
        except Exception as e:
            logger.error(f'Error improving text with Ollama: {str(e)}')
            return jsonify({'error': 'Error improving text'}), 500

        return jsonify({
            'raw_text': extracted_text,
            'improved_text': improved_text
        }), 200

    except Exception as e:
        logger.error(f'General error in OCR endpoint: {str(e)}')
        return jsonify({'error': f'General error: {str(e)}'}), 500

    finally:
        # Clean up the temporary image
        if img_path and os.path.exists(img_path):
            try:
                os.remove(img_path)
            except Exception as e:
                logger.error(f'Error removing temporary image: {str(e)}')
\end{lstlisting}

This endpoint accepts image data in both JSON and multipart/form-data formats. It decodes the image, validates its format, and processes it using Optical Character Recognition (OCR). The raw text extracted from the image is then enhanced through the Ollama API, and both the original and improved texts are returned as part of the response. Additionally, the endpoint manages errors effectively and ensures that temporary image files are cleaned up after processing.

The following utility functions play a crucial role in the OCR process:
\begin{itemize}
    \item \texttt{allowed\_file}: Verifies whether the uploaded file has an allowed extension.
    \item \texttt{improve\_text\_with\_ollama}: Sends the extracted text to the Ollama API for enhancement.
    \item \texttt{pytesseract.image\_to\_string}: Extracts text from images using the Tesseract OCR engine.
\end{itemize}

These functions are essential for processing image data, interfacing with external APIs, and ensuring the accuracy and reliability of the OCR workflow.

\subsection{Programming Bot Endpoints}

The Programming Bot endpoints manage all requests related to the Programming Bot functionality within the Student AI Hub and the Code Extension. These endpoints route user queries to the Ollama API and return code-based responses, tailored specifically for programming inquiries.

The following code snippet illustrates the endpoint for the Programming Bot:

\begin{lstlisting}[language=Python, caption={Programming Bot Endpoint}]
@chat_bp.route('/ask_programming_bot', methods=['POST'])
def ask_programming_bot_endpoint():
    """
    Programming Bot Endpoint: Receives a user query, communicates with Ollama, and returns the response.
    """
    data = request.get_json()

    if not data:
        logger.warning('No JSON data provided')
        return jsonify({'error': 'No JSON data provided'}), 400

    prompt = data.get('prompt')
    model = data.get('model', 'qwen2.5-coder:0.5b')

    if not prompt:
        logger.warning('Prompt missing')
        return jsonify({'error': 'Prompt missing'}), 400

    try:
        # Define a specialized system message for the programming bot
        messages = [
            {
                'role': 'system',
                'content': (
                    "You are an experienced programmer and technical advisor named Luminara. "
                    "Answer programming questions precisely and provide clear code examples in the requested programming language."
                )
            },
            {
                'role': 'user',
                'content': prompt
            }
        ]

        # Send the request to Ollama
        response = chat(
            model=model,
            messages=messages,
            stream=False
        )

        # Extract and clean the response
        bot_response = response.message.content.strip()

        if not bot_response:
            logger.error('Ollama did not return a response.')
            raise Exception('Ollama could not generate a response.')

        return {'choices': [{'text': bot_response}]}

    except ResponseError as e:
        logger.error(f'Ollama ResponseError: {e.error}')
        return jsonify({'error': f'Ollama API error: {e.error}'}), 500
    except Exception as e:
        logger.error(f'Ollama error: {str(e)}')
        return jsonify({'error': str(e)}), 500 
\end{lstlisting}

This endpoint follows a structure similar to other endpoints but includes modifications to the prompt to enhance the quality of the AI's response. A system message informs the AI that it is interacting with an experienced programmer and technical advisor named Luminara, which guides it to provide more accurate and context-specific answers. The endpoint handles the connection to the Ollama API, processes the response, and ensures that errors are logged and appropriately returned to the user.

\section{Utility Functions}

The utility functions presented in this section are fundamental to the operation of the Flask service. They execute critical tasks—such as interfacing with external APIs, processing data, and managing server resources—in a modular, reusable, and efficient manner. This design ensures that the service operates reliably and seamlessly.

\subsection{Text Processing Utilities}

The text processing utilities are used to enhance the quality and readability of text extracted from images or user queries. They leverage the Ollama API to refine the text, correct errors, and improve coherence, thereby enhancing the overall user experience.

\paragraph{ask\_ollama}

The \texttt{ask\_ollama} function facilitates communication with the Ollama API to generate responses for user queries. It accepts a prompt along with an optional model parameter, dispatches the request to the Ollama API, and returns the resulting response.

The code snippet below demonstrates the implementation of the \texttt{ask\_ollama} utility function:

\begin{lstlisting}[language=Python, caption={ask\_ollama Utility Function}]
def ask_ollama(prompt, model='llama3.2:1b'):
    """
    Sends the user prompt to the selected Ollama model and returns the response.
    """
    try:
        # Define the message structure
        messages = [
            {
                'role': 'system',
                'content': 'You are a helpful assistant. Please answer the following user query:'
            },
            {
                'role': 'user',
                'content': prompt
            }
        ]

        # Send the request to Ollama
        response = chat(
            model=model,
            messages=messages,
            stream=False
        )

        # Extract the response content
        bot_response = response.message.content.strip()

        if not bot_response:
            logger.error('Ollama did not return any response.')
            raise Exception('Ollama was unable to generate a response.')

        return {'choices': [{'text': bot_response}]}

    except ResponseError as e:
        logger.error(f'Ollama ResponseError: {e.error}')
        return {'error': f'Ollama API error: {e.error}'}
    except Exception as e:
        logger.error(f'Ollama error: {str(e)}')
        return {'error': str(e)}
\end{lstlisting}

This function encapsulates the logic necessary for transmitting user prompts to the Ollama API and processing the responses. It is engineered to handle diverse user queries, thereby generating accurate and contextually relevant answers. Consequently, it serves as a critical component underpinning the chatbot functionality of the Flask service.

\paragraph{ask\_ollama\_vision}

The \texttt{ask\_ollama\_vision} function augments the capabilities of the standard \texttt{ask\_ollama} utility by incorporating support for image-based queries. It processes image data, integrates it with textual prompts, and dispatches the combined input to the Ollama Vision API for response generation. This extension enables the Flask service to handle multimodal inputs, thereby broadening its applicability and enhancing user interaction.

The following code snippet demonstrates the implementation of the \texttt{ask\_ollama\_vision} function:

\begin{lstlisting}[language=Python, caption={ask\_ollama\_vision Utility Function}]
def ask_ollama_vision(prompt, model='llama3.2-vision', image_paths=[]):
    """
    Dispatches the user's prompt along with associated images to the selected Ollama Vision model,
    and returns the generated response.
    """
    try:
        # Define the message structure with associated image paths
        messages = [
            {
                'role': 'user',
                'content': prompt,
                'images': image_paths  # Attach image paths to the user message
            }
        ]

        # Send the request to Ollama Vision
        response = chat(
            model=model,
            messages=messages,
            stream=False
        )

        # Extract the response content
        bot_response = response.message.content.strip()

        if not bot_response:
            logger.error('Ollama did not return any response.')
            raise Exception('Ollama was unable to generate a response.')

        return {'choices': [{'text': bot_response}]}

    except ResponseError as e:
        logger.error(f'Ollama ResponseError: {e.error}')
        return {'error': f'Ollama API error: {e.error}'}
    except Exception as e:
        logger.error(f'Ollama error: {str(e)}')
        return {'error': str(e)}
\end{lstlisting}

This function is instrumental in enabling the Flask service to process and interpret image data in conjunction with textual prompts. By integrating image recognition capabilities, it enhances the service's versatility and responsiveness, thereby significantly enriching the overall user experience.

\paragraph{improve\_text\_with\_ollama}

The \texttt{improve\_text\_with\_ollama} function is designed to refine text extracted through the OCR process by leveraging the Ollama API. It improves readability, grammar, and coherence, converting raw text into a well-structured Markdown format that is both clear and professionally formatted.

The code snippet below illustrates the implementation of the \texttt{improve\_text\_with\_ollama} function:

\begin{lstlisting}[language=Python, caption={improve\_text\_with\_ollama Utility Function}]
def improve_text_with_ollama(text, model='llama3.2'):
    """
    Enhances the provided text using the Ollama API and returns the refined Markdown text.
    """
    try:
        # Define the message structure with system and user roles to improve the text
        messages = [
            {
                'role': 'system',
                'content': (
                    "You are a highly proficient text processor and Markdown expert. "
                    "Your task is to enhance the following text, which has been extracted via OCR from an image, "
                    "and convert it into a well-structured Markdown format. "
                    "Ensure correct spelling, grammar, and syntax, and format the text using appropriate Markdown elements "
                    "such as headings, lists, code blocks, and emphasis. "
                    "Provide only the enhanced Markdown text without any additional commentary or explanations."
                )
            },
            {
                'role': 'user',
                'content': (
                    "Here is the text to be processed:\n\n"
                    f"```\n{text}\n```"
                )
            }
        ]

        # Send the request to Ollama
        response = chat(
            model=model,
            messages=messages,
            stream=False
        )

        # Retrieve the improved text from the response
        markdown_text = response.message.content.strip()

        if not markdown_text:
            logger.error('Ollama did not return any text.')
            raise Exception('Ollama was unable to improve the text.')

        return markdown_text

    except ResponseError as e:
        logger.error(f'Ollama ResponseError: {e.error}')
        raise e
    except Exception as e:
        logger.error(f'Ollama error: {str(e)}')
        raise e
\end{lstlisting}

This utility function plays a pivotal role in enhancing the quality and coherence of OCR-extracted content. By refining the raw text into a structured Markdown format, it ensures that the final output is both accurate and user-friendly, thereby improving the overall efficacy of the OCR workflow.


\subsection{OCR (Optical Character Recognition) Utilities}

This section details the OCR utilities integrated within the Flask service, which are responsible for processing images and extracting textual content. Currently, the primary function implemented is \texttt{perform\_ocr}, which utilizes the \texttt{pytesseract} library to perform OCR operations.

\paragraph{perform\_ocr}

The \texttt{perform\_ocr} function harnesses the Tesseract OCR engine to extract text from an image file. It accepts an image file path and an optional language parameter, processes the image, and returns the extracted text. The function is designed to handle a variety of image formats and ensure accurate text retrieval, thus streamlining the OCR workflow within the Flask service.

The code snippet below demonstrates the implementation of the \texttt{perform\_ocr} function:

\begin{lstlisting}[language=Python, caption={perform\_ocr Utility Function}]
import pytesseract
from PIL import Image

def perform_ocr(image_path, lang='deu'):
    """
    Performs OCR on the specified image file and returns the extracted text.
    """
    try:
        # Load the image and convert it to grayscale
        image = Image.open(image_path).convert('L')  # Convert to grayscale

        # Optional: Apply image preprocessing techniques (e.g., denoising, thresholding, etc.)
        custom_config = r'--oem 3 --psm 6'
        text = pytesseract.image_to_string(image, lang=lang, config=custom_config)
        return text
    except Exception as e:
        logger.error(f'OCR error: {str(e)}')
        raise e
\end{lstlisting}

This function encapsulates the essential logic for image processing and text extraction using the Tesseract OCR engine. By leveraging robust libraries, it ensures that the Flask service can efficiently convert image-based data into readable text.

The key libraries utilized are:

\textbf{pytesseract:} A Python wrapper for Google's Tesseract-OCR Engine, used for extracting text from images via optical character recognition (OCR).

\textbf{PIL (Python Imaging Library):} An image processing library that enables the opening, manipulation, and saving of various image file formats, thereby enhancing Python's capabilities in handling visual data.

\section{Deployment}

The Flask service was deployed across multiple platforms to support comprehensive testing, evaluation, and integration with other system components. The deployment process involved configuring the server environment, establishing required dependencies, and ensuring seamless operation across various hosting infrastructures.

For development and testing, the Flask service is deployed locally on a laptop or desktop machine. This setup enables rapid iteration, feature testing, and effective troubleshooting in a controlled environment. Local deployment requires installing Python, setting up a virtual environment, and running the Flask server on the local system.

In production environments, the Flask service is hosted on dedicated servers or cloud platforms to ensure high availability, scalability, and reliability. This approach enables the service to handle a high volume of requests and users efficiently. Production deployment typically involves meticulous server configuration, dependency management, and the implementation of robust security measures to mitigate potential threats.

To further streamline production deployment, a Docker container was developed. More information on the Docker-based deployment approach is provided in Section \ref{sec:docker}.

\section{Docker}
\label{sec:docker}
Docker is a platform that allows you to run applications in containers. A container is like a small, isolated environment where software runs with everything it needs, including the operating system, libraries, and dependencies.

No matter which computer or server the container runs on, it always works the same way. This means you do not have to worry about an application suddenly throwing errors on a different system just because a different software version is installed there.

Docker is often used in software development and cloud applications because it simplifies testing, deployment, and scaling of apps. Developers can store their software as images and share them with others without requiring complicated installations.

\subsection{Used Docker Images}
A Docker image is a blueprint that specifies how to run the application. The instructions for the build are stored in the Dockerfile.
\cite{dockerize_flask} 

\begin{itemize}
    \item \textbf{Flask Application} The Flask image is used to easily implement the Flask application in a Docker container.
    \item \textbf{ollama} The Ollama image is used to avoid running LLMs globally and use them in a secluded environment.
\end{itemize}

\subsection{Docker Compose}
Docker Compose is used for running multiple containers at the same time. It simplifies your application and makes it easier to manage 
The Configuration is stored in a single YAML file. All the services can be started with a simple command. It is a very compact way to manage Docker applications.
\cite{docker_compose} 

\author{Florian Prandstetter}

\section{Scalability and Performance Concerns}

One notable limitation of the Flask server is its inherent lack of scalability. Flask, being primarily designed for lightweight applications, is not optimized for handling high volumes of concurrent requests. In our implementation, the server was deployed on a modest PC with limited computational resources. Consequently, if the service were to be deployed in a production environment, it would be imperative to migrate to more robust hardware or consider a distributed, multi-server architecture to effectively manage the anticipated load. Given the constraints of the project timeline and the prototype nature of this work, scalability was not prioritized during development.


\section{Conclusion and Future Work}

In summary, this chapter has detailed the architecture, functionality, and deployment strategy of the self-hosted Flask service. 
The design emphasizes modularity and expandability, allowing for rapid prototyping, tailored configurations, and seamless integration with external APIs. 
The clear separation of concerns across endpoints and utility functions ensures both maintainability and flexibility, which are essential for evolving project requirements.

However, practical limitations regarding scalability and performance on modest hardware were also noted. 
Addressing these challenges through distributed architectures and enhanced concurrency mechanisms will be crucial for future deployments. 
As a result, future work should focus on optimizing resource management, scaling the service to meet higher demand, and further refining API integrations for improved robustness and security.

The next two chapters will delve into the front-end components of the Student AI Hub and the code extension, providing a comprehensive overview of their design, functionalities, and integration with the Flask service.



