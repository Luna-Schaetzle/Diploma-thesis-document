\chapter{hosted Flask Service}
\label{cha:hosted_flask_service}
\textbf{Author:} Luna P. I. Schätzle

\textbf{Author:} Florian Prandstetter (docker section \ref{sec:docker})

This chapter delineates the implementation, architecture, and deployment of the self-hosted Flask service, 
which functions as a pivotal interface between the front-end and back-end components of the system. 
In addition to detailing the technical design, the chapter critically examines the advantages of a self-hosted solution 
and the rationale behind key architectural decisions.

\section{Introduction}
This section offers a comprehensive overview of the motivations underpinning the development of the Flask service. 
Functioning as the backbone for both the Student AI Hub and the code extension’s backend, 
the service was conceived to address a range of specific operational requirements. 
Here, we elaborate on the core functionalities of the service, detail the technical imperatives that drove its inception, 
and position its role within the broader system architecture, thereby laying the groundwork for subsequent technical discussions.

\section{Advantages of a Self-hosted Service}
A self-hosted service confers a multitude of benefits relative to externally managed or third-party solutions. This section examines these advantages in depth:
\begin{itemize}
    \item \textbf{Enhanced Customization and Environmental Control:} By hosting the service internally, developers gain complete authority over the configuration and optimization of the operating environment. This control facilitates the implementation of domain-specific modifications and enables precise tuning to meet the unique needs of the project.
    \item \textbf{Rapid Prototyping and Agile Deployment:} The self-hosted nature of the service supports agile development practices. New features and bespoke functionalities can be rapidly prototyped, iteratively tested, and deployed, thereby significantly reducing development cycles and accelerating time-to-market.
    \item \textbf{Improved Data Security and Regulatory Compliance:} Hosting the service in-house allows for stringent oversight of data management practices. This approach is particularly advantageous in contexts governed by strict data protection regulations and institutional policies, as it enables the implementation of tailored security measures and enhances overall control over sensitive information.
\end{itemize}
Collectively, these factors validate the strategic decision to pursue a self-hosted approach, underscoring its technical, operational, and regulatory merits.

\section{Flask as a Web Framework}

Flask is a lightweight, yet versatile web framework that plays a central role in the development of our Student AI Hub. Its minimalistic design and powerful capabilities make it an excellent choice for both rapid prototyping and scalable application development.

\subsection{Core Functionalities of Flask}

Flask's architecture is built around several core functionalities that streamline web application development:
\begin{itemize}
    \item \textbf{Request Routing:} Flask’s routing system allows for the efficient mapping of URLs to Python functions. This ensures that incoming web requests are correctly handled and directed to the appropriate endpoints, which is crucial for managing user interactions.
    \item \textbf{Templating Engine:} By leveraging the Jinja2 templating engine, Flask enables dynamic content generation. This allows developers to create flexible HTML templates that can be easily populated with data, ensuring a seamless user experience.
    \item \textbf{Middleware Support:} Flask supports middleware, facilitating the insertion of additional processing layers before or after a request is handled. This capability is essential for tasks such as authentication, logging, and error handling.
    \item \textbf{Extensibility:} Its modular design allows developers to integrate third-party extensions or develop custom modules, thereby expanding Flask's functionalities to meet the evolving demands of the project.
    \item \textbf{Simplicity and Clarity:} Flask’s clear and concise API encourages clean, maintainable code. This simplicity does not come at the expense of power; rather, it provides a robust foundation upon which complex systems can be built.
\end{itemize}

These features collectively contribute to an efficient workflow, ensuring that web requests and responses are managed in a structured and scalable manner.

\subsection{Rationale for Selecting Flask}

The decision to adopt Flask as the backbone of our web service was influenced by several key considerations:
\begin{itemize}
    \item \textbf{Simplicity and Flexibility:} Flask’s minimalistic core allows for rapid development and iterative prototyping without the overhead of a full-stack framework. This flexibility is invaluable during the early stages of project development and testing.
    \item \textbf{Extensive Documentation and Community Support:} With comprehensive documentation and a vibrant community, Flask offers abundant resources for troubleshooting and extending functionality. This support network accelerates development and helps resolve challenges efficiently.
    \item \textbf{Scalability:} Despite its simplicity, Flask is designed to scale. Its modular nature means that as the project grows, new features can be seamlessly integrated, ensuring that the framework remains robust even under increased demand.
    \item \textbf{Integration Capabilities:} Flask can be easily combined with various libraries and APIs. This makes it particularly well-suited for integrating AI models, data processing tools, and other external services—core requirements of the Student AI Hub.
\end{itemize}

In summary, Flask’s combination of ease-of-use, extensibility, and strong community support provides a solid and adaptable framework for developing a dynamic web service. Its ability to handle everything from simple routing to complex middleware interactions makes it the ideal choice for building a robust backend that meets the diverse needs of our project.

\section{Architecture and Service Structure}

The Flask service functions as a robust and flexible backend for the Student AI Hub and the code extension. Its modular and extensible design facilitates the seamless integration of additional functionalities and services. The figure below illustrates the key architectural components of the Flask service and their interactions.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/flask_service.png}
    \caption{Flask Service Architecture}
    \label{fig:flask_service_architecture}
\end{figure}

\subsection{System Architecture}

The system is organized around a central Python script, \texttt{app.py}, which handles incoming requests, registers endpoints, initiates the server, and manages logging. Server configuration is maintained in \texttt{config.py}, where settings such as the language model, image processing rules, upload folder, and conversion language are defined.

Endpoints are organized within the \texttt{routes} subfolder, with similar endpoints grouped together. They are divided into the following files:
\begin{itemize}
    \item \textbf{chatbot\_route.py:} Contains endpoints related to chatbot functionality.
    \item \textbf{OCR\_route.py:} Manages endpoints for image to Text conversion.
    \item \textbf{Programming\_bot\_route.py:} Houses endpoints for the programming bot.
\end{itemize}

Within these files, endpoints process user requests and return responses, and are designed for easy extension. They also handle error management and user feedback by converting data from the user and the Ollama API into the appropriate format when necessary. Connections to the Ollama API and the OCR (Optical Character Recognition) system are managed within the \texttt{Utils} folder, which contains all utility functions used by the endpoints, including those for sending requests, performing OCR, and converting Markdown to HTML.

\subsection{Expandability and Modularity}

The server is designed for easy expandability and modularity. Endpoints are stored in separate files, which allows for the straightforward addition of new functionality. Similarly, utility functions can be updated or replaced with minimal effort, and the server configuration is flexible enough to accommodate new parameters.

This modular and extensible architecture ensures that the Flask service can readily adapt to future enhancements and integrations. These design principles contribute to a flexible, maintainable server that can be effortlessly updated with new features and functionalities.

\section{RESTful Endpoints and Functionalities}
\label{sec:endpoints}

This section provides a comprehensive overview of the RESTful endpoints implemented in the Flask service. Each endpoint is detailed in terms of its input parameters, expected output, and the underlying logic that processes incoming requests.

\subsection{Chatbot Endpoints}

The Chatbot endpoints are responsible for handling all server requests associated with both the general chatbot and the image recognition chatbot.

\paragraph{Chatbot Endpoint}
The following code snippet illustrates the endpoint for the general chatbot:

\begin{lstlisting}[language=Python]
@chat_bp.route('/ask_ollama', methods=['POST'])  # Defines the endpoint
def ask_ollama_endpoint():
    """
    Chat endpoint: Receives a user query and a model, communicates with Ollama, and returns the response.
    """
    data = request.get_json()

    if not data:
        logger.warning('No JSON data provided')
        return jsonify({'error': 'No JSON data provided'}), 400

    prompt = data.get('prompt')
    model = data.get('model', Config.OLLAMA_MODEL_DEFAULT)  # Default model

    if not prompt:
        logger.warning('Prompt missing')
        return jsonify({'error': 'Prompt missing'}), 400

    try:
        response = ask_ollama(prompt, model=model)

        if 'choices' in response:
            return jsonify(response), 200
        else:
            return jsonify(response), 500

    except Exception as e:
        logger.error(f'Error communicating with Ollama: {str(e)}')
        return jsonify({'error': 'Error communicating with Ollama'}), 500
\end{lstlisting}

This endpoint extracts the necessary data from the API request and passes it to the \texttt{ask\_ollama} utility function, which communicates with the Ollama API and returns the corresponding response. If any required data is missing or if the Ollama API is unreachable, the endpoint returns an appropriate error message.

\paragraph{Image Recognition Endpoint}
The following code snippet demonstrates the endpoint for the Image Recognition Chatbot:

\begin{lstlisting}[language=Python]
@chat_bp.route('/ask_ollama_vision', methods=['POST'])  # Defines the endpoint using POST method
def ask_ollama_vision_endpoint():
    """
    Vision Chat endpoint: Receives a user query with images and a model, communicates with Ollama Vision, and returns the response.
    """
    data = request.get_json()

    if not data:
        logger.warning('No JSON data provided')
        return jsonify({'error': 'No JSON data provided'}), 400

    prompt = data.get('prompt')
    model = data.get('model', 'llama3.2-vision')  # Default model for vision tasks
    images = data.get('images', [])

    if not prompt and not images:
        logger.warning('Neither prompt nor images provided')
        return jsonify({'error': 'Neither prompt nor images provided'}), 400

    # Process the images
    image_paths = []
    try:
        for idx, img_str in enumerate(images):
            # Extract image data from base64 string
            if ',' in img_str:
                header, encoded = img_str.split(',', 1)
            else:
                header, encoded = '', img_str
            img_data = base64.b64decode(encoded)
            img = Image.open(BytesIO(img_data))

            # Validate the image format
            if img.format.lower() not in Config.ALLOWED_EXTENSIONS:
                logger.warning(f'Invalid image format: {img.format}')
                return jsonify({'error': f'Invalid image format: {img.format}'}), 400

            # Optionally reduce or compress image size
            img.thumbnail((1024, 1024))  # Example: maximum size 1024x1024

            # Save the image temporarily
            img_filename = f"temp_{idx}.{img.format.lower()}"
            img_path = os.path.join(Config.UPLOAD_FOLDER, img_filename)
            img.save(img_path)
            image_paths.append(img_path)

    except Exception as e:
        logger.error(f'Error processing images: {str(e)}')
        return jsonify({'error': f'Error processing images: {str(e)}'}), 400

    try:
        response = ask_ollama_vision(prompt, model=model, image_paths=image_paths)

        if 'choices' in response:
            return jsonify(response), 200
        else:
            return jsonify(response), 500

    except Exception as e:
        logger.error(f'Error communicating with Ollama Vision: {str(e)}')
        return jsonify({'error': 'Error communicating with Ollama Vision'}), 500

    finally:
        # Delete temporary images
        for img_path in image_paths:
            if os.path.exists(img_path):
                os.remove(img_path)
\end{lstlisting}

The Image Recognition endpoint functions similarly to the general chatbot endpoint, with the added capability of processing image data. It decodes the base64-encoded images, saves them temporarily, validates their format, and resizes them if necessary to optimize processing speed and reduce server load. Once processed, the images are passed to the \texttt{ask\_ollama\_vision} utility function. After the request is handled, the endpoint ensures that all temporary images are deleted.

Overall, these endpoints are designed for scalability and robustness, efficiently managing both text and image inputs while providing meaningful feedback to the user in case of errors.

% ################################################################

\subsection{OCR Endpoints}



\subsection{Programming Bot Endpoints}

\subsection{Code Illustrations}
To enhance understanding, code examples are provided to demonstrate the implementation of key endpoints. These examples highlight the methods used to process requests and generate responses.


\subsection{Utility Functions}
An overview of auxiliary utility functions is given, focusing on their roles in logging, data validation, and error handling. These functions contribute to the overall robustness and reliability of the service.


\section{Utilized Libraries}
A comprehensive inventory of the external libraries used in the project is presented in this section. For each library, its functionality, role within the project, and integration aspects are discussed.


\section{Deployment}

% How we deployed the Server on a Laptop, The School Server and on our Server via Docker
% Explain how we Deployed the Server on a Laptop, The School Server and on our Server via Docker (Docker gets Explained in the next Section)

%###################

\section{Docker}
\label{sec:docker}
Docker is a platform that allows you to run applications in containers. A container is like a small, isolated environment where software runs with everything it needs – including the operating system, libraries, and dependencies.

No matter which computer or server the container runs on, it always works the same way. This means you don’t have to worry about an application suddenly throwing errors on a different system just because a different software version is installed there.

Docker is often used in software development and cloud applications because it simplifies testing, deployment, and scaling of apps. Developers can store their software as images and share them with others without requiring complicated installations.

\subsection{Used Docker Images}
A docker image is a blueprint that specifies how to run the application. The instructions for the build are stored in the Dockerfile.
\cite{dockerize_flask} 

\begin{itemize}
    \item \textbf{flask\_app} The Flask image is used to easily implement the Flask application in a Docker container.
    \item \textbf{ollama} The Ollama image is used to avoid running LLMs globally and use them in a secluded environment.
\end{itemize}

\subsection{Docker Compose}
Docker Compose is used for running multiple containers at the same time. It simplifies your application and makes it easier to manage 
The Configuration is stored in a single YAML file. All the services can be started with a simpel command. It is a very compact way to manage Docker application.
\cite{docker_compose} 

\author{Florian Prandstetter}

\section{Scalability and Performance Concerns}

One notable limitation of the Flask server is its inherent lack of scalability. Flask, being primarily designed for lightweight applications, is not optimized for handling high volumes of concurrent requests. In our implementation, the server was deployed on a modest PC with limited computational resources. Consequently, if the service were to be deployed in a production environment, it would be imperative to migrate to more robust hardware or consider a distributed, multi-server architecture to effectively manage the anticipated load. Given the constraints of the project timeline and the prototype nature of this work, scalability was not prioritized during development.

% ########################################

\section{Conclusion and Future Work}
This concluding section synthesizes the chapter’s key points and reflects on the efficacy of the implemented service. It also outlines potential avenues for future enhancements, such as further scalability improvements, additional functionalities, and more robust deployment automation.





