\chapter{Overview and Integration of AI Models}
\label{cha:Introduction_to_the_used_AI_Models}

For the Diploma thesis, there are many different AI models that are in use. There are different Types of AI models, such as:
\begin{itemize}
    \item LLMs (Large Language Models)
    \item Defusion Models (Models that are used to create images)
    \item Object Detection Models (Models that are used to detect objects in images)
    \item Face Recognition Models (Models that are used to recognize faces in images)
\end{itemize}

In the following chapters, the different Types and the used models will be explained in more detail.

\section{Large Language Models (LLMs)}

Large Language Models (LLMs) represent a significant advancement in artificial intelligence, enabling machines to process and generate natural language. 
LLMs are built on the concept of deep learning, utilizing neural networks with billions of parameters to understand and generate text in a contextually accurate 
and coherent manner. These models are trained on vast datasets encompassing diverse topics, allowing them to handle a wide range of tasks, such as translation, 
summarization, content generation, and conversational AI.

\subsection{Key Characteristics of LLMs}

\begin{itemize}
\item \textbf{Scale and Complexity:} LLMs are distinguished by their immense size, often containing billions of parameters, enabling them to capture intricate patterns in language.
\item \textbf{Transfer Learning:} These models benefit from pretraining on large datasets, followed by fine-tuning for specific tasks, making them highly versatile.
\item \textbf{Contextual Understanding:} LLMs excel at understanding context, which allows them to generate coherent and contextually appropriate responses.
\item \textbf{Multilingual Capabilities:} Many LLMs are trained on datasets in multiple languages, enabling them to process and generate text in various languages.
\end{itemize}

\subsection{Applications of LLMs}

\begin{itemize}
\item Text summarization and paraphrasing.
\item Question answering and information retrieval.
\item Conversational agents and chatbots.
\item Code generation and debugging assistance.
\item Creative writing, including story and poetry generation.
\end{itemize}

\subsection{Examples of Popular LLMs}

\begin{itemize}
\item \textbf{GPT Models:} Developed by OpenAI, these models include GPT-3, GPT-4, and ChatGPT, known for their state-of-the-art performance in text generation and comprehension.
\item \textbf{BERT (Bidirectional Encoder Representations from Transformers):} Developed by Google, BERT focuses on understanding context by analyzing text bidirectionally.
\item \textbf{LLama Models:} Created by Meta, these models are designed for efficient natural language understanding and generation.
\item \textbf{Mistral Models:} Aimed at specialized tasks with high precision and multilingual capabilities.
\end{itemize}

\subsection{Advantages and Challenges of LLMs}

\textbf{Advantages:}
\begin{itemize}
\item High accuracy in generating and understanding text.
\item Adaptability to a variety of domains and languages.
\item Ability to process complex and context-rich queries.
\end{itemize}

\textbf{Challenges:}
\begin{itemize}
\item High computational and memory requirements.
\item Potential biases due to the training data.
\item Difficulty in maintaining factual accuracy in generated content.
\end{itemize}

\cite{what-are-LLMs-IBM}

\section{Utilized Large Language Models}

In the context of this diploma thesis, various free and commercial large language models (LLMs) 
were evaluated to determine their suitability for integration. Leveraging the Ollama application, 
we were able to test and compare several LLMs. Additionally, we explored different ChatGPT models available through the OpenAI API.
OpenAI offers a range of models that vary in terms of size and complexity, with more advanced models incurring higher usage costs.

\cite{OpenAI_API_overview}
\cite{WhatisOllama}

\section{Ollama Application Overview}

% Make an introduction to Ollama
Explain what ollama is and what it does

\subsection{Ollama Features}

\subsection{Ollama Architecture}

\subsection{Ollama Models}

\subsection{Ollama API}

\subsection{Ollama Use Cases} % eventuell nicht 

\subsection{Ollama Integration}

\subsection{Benefits and Limitations of Ollama}



\section{Models Accessed via Ollama}

For this project, we tested multiple models available through the Ollama application, 
which can be obtained via the Ollama download server.

Since Ollama operates locally, the selected models needed to meet specific requirements to ensure efficient performance. 
Consequently, we evaluated models of varying sizes and levels of complexity.

\subsection{Model Selection Criteria}

The selection of models was guided by the following criteria:

\begin{itemize}
\item \textbf{Model Size:} The model must be capable of running on the server without exceeding available memory capacity.
\item \textbf{Performance Speed:} The response time of the model, i.e., how quickly it can generate output.
\item \textbf{Complexity:} The model's ability to handle complex prompts and generate coherent, contextually accurate text.
\item \textbf{Accuracy:} The overall precision of the model's responses, particularly in terms of factual correctness and linguistic quality.
\item \textbf{Language Support:} The model's proficiency in understanding and generating text in multiple languages, particularly English and German.
\item \textbf{User Experience:} The model's overall usability and user-friendliness, including ease of integration and customization.
\end{itemize}

There is often a trade-off between these criteria. 
Larger models tend to exhibit higher accuracy and greater contextual understanding but are generally slower and 
require more computational resources.

\subsection{Challenges in Model Testing and Updates}

One significant challenge lies in the rapid development and frequent release of new models, 
which complicates the process of continuous integration and comprehensive evaluation of recent advancements. 
Regular testing and updates are imperative to ensure the incorporation of state-of-the-art models while maintaining system reliability and relevance.

Another challenge involves achieving an optimal balance between performance, accuracy, and resource efficiency, 
ensuring that the chosen model meets the application’s functional requirements without compromising the overall user experience.

During the evaluation process, we encountered several obstacles. For instance, 
identifying standardized questions that could be uniformly answered by all models proved challenging. 
Some smaller models demonstrated limitations in addressing certain questions comprehensively. Additionally, 
specialized models, while excelling in niche areas, often lacked the ability to provide detailed answers across a broader range of topics.

For the final evaluation phase, we employed a diverse question set consisting of self-crafted questions, 
publicly available questions from online sources, and queries generated by ChatGPT-4 to ensure a comprehensive assessment covering a wide spectrum of queries.

\subsection{Model Selection for the Final Application}

In the final implementation, users are provided with a curated list of recommended models from which they can select their preferred option. 
This list was carefully compiled based on our comprehensive testing and reflects the models that demonstrated 
the best balance between performance, accuracy, and resource efficiency.

For the production version of the application, 
this list must be updated periodically to include newly released models and maintain optimal performance.

\subsection{!!!Models Evaluated During Testing}

A detailed comparison of the models tested during the evaluation phase is provided in the following sections, 
highlighting their respective strengths and limitations.

\subsection{!!!Models Integrated into the Final Application}

The final selection of models integrated into the application reflects the outcomes of our performance benchmarks and user-centric assessments, 
ensuring a robust and adaptable solution for end-users.


\section{Ollama Model Testing and Evaluation}

Based on the aforementioned criteria, we conducted an extensive evaluation of the following models:

\subsection{Quantitative Evaluation Methods}

For the quantitative evaluation, we focused on key performance metrics to assess the efficiency and reliability of each model:

\begin{itemize}
    \item \textbf{Response Time:} The time taken by the model to generate a response after receiving input.
    \item \textbf{CPU Usage:} The percentage of CPU resources utilized during model execution.
    \item \textbf{GPU Usage:} The extent to which GPU resources were leveraged to enhance performance.
    \item \textbf{Memory Usage:} The amount of RAM consumed while the model was running.
    \item \textbf{Multiple Choice Question Answering:} The accuracy of the model when answering structured multiple-choice questions.
    \item \textbf{Translation Quality:} Measured using the BLEU (Bilingual Evaluation Understudy) score, which evaluates the similarity between the model-generated translation and a human reference translation. \cite{BLUE-Score-Wikipedia}
    \item \textbf{Text Generation Quality:} Assessed using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score to measure the overlap between the generated text and reference texts.
\end{itemize}

\cite{ROUGE-BLUE-Score}


\subsection{Qualitative Evaluation Methods}

Qualitative evaluation requires human judgment and is inherently resource-intensive. Therefore, our primary focus was placed on the quantitative evaluation. Nevertheless, we conducted qualitative assessments for specific criteria where human input was indispensable:

\begin{itemize}
    \item \textbf{Contextual Understanding:} The model's ability to interpret and respond accurately to the context of the input text.
    \item \textbf{Factual Correctness:} The accuracy of the factual information provided in the model's output.
    \item \textbf{Linguistic Quality:} The grammatical correctness, fluency, and coherence of the generated text.
    \item \textbf{Emotional Intelligence:} The extent to which the model's output mimics human-like empathy, tone, and emotional nuance.
\end{itemize}

By combining both quantitative and qualitative evaluation methods, 
we obtained a comprehensive understanding of each model's strengths and limitations.


\subsection{Models Evaluated During Testing}

For the evaluation process, we selected the most popular models available in the Ollama application and conducted extensive testing on each.

\begin{itemize}
    \item \textbf{qwen2.5-coder:0.5b} - A compact model with 0.5 billion parameters, specifically designed for coding tasks.
    \item \textbf{qwen2.5-coder:7b} - A small-scale model with 7 billion parameters, optimized for coding tasks.
    \item \textbf{qwen2.5-coder:14b} - A medium-sized model with 14 billion parameters, tailored for coding-related tasks.
    \item \textbf{llama3.2:1b} - A small model with 1 billion parameters, designed for general-purpose tasks by Meta.
    \item \textbf{llama3.2:2b} - A medium model with 2 billion parameters, developed for general-purpose tasks by Meta.
    \item \textbf{mistral:7b} - A medium-sized model with 7 billion parameters, created for general tasks by Mistral AI, a European AI company.
    \item \textbf{phi4:14b} - A medium-sized model with 14 billion parameters, intended for general tasks by Microsoft.
    \item 
\end{itemize}

\cite{Ollama-models-overview}

%#############################################################################################################

\subsection{Data Collection}

For the Data collection we used the School AI Server wich was provided by the HTL Anichstraße, to run the different models in the Evaluation phase.
For the later Data collection we used our own Server to run the different models.


To colletct the data we used a varaitey of different python scripts. For the quantitative data we used the following python script:

\begin{lstlisting}[style=Python, caption={Python-quantitative-data-collection}, captionpos=b]
import time
import json
import psutil  # For CPU, memory usage
from ollama import chat

# Prompts for testing
prompts = [
    "Explain the theory of relativity in simple terms.",
    "Create a short story about a knight.",
    "What are the advantages of open-source projects?",
    "Write a Python function that outputs prime numbers up to 100.",
    # ...
]

# Model name
model_name = "qwen2-math"

# Store results
results = []

# Function to get GPU usage if available
def get_gpu_usage():
    try:
        import torch
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB
            gpu_utilization = torch.cuda.utilization(0) if hasattr(torch.cuda, 'utilization') else "N/A"
            return gpu_memory, gpu_utilization
        else:
            return 0, "No GPU detected"
    except ImportError:
        return 0, "torch not installed"

# Loop through prompts
for prompt in prompts:
    try:
        # Measure system usage before model execution
        cpu_before = psutil.cpu_percent(interval=None)
        memory_before = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB

        start_time = time.time()
        # Ollama chat request
        response = chat(model=model_name, messages=[{'role': 'user', 'content': prompt}])
        end_time = time.time()

        latency = end_time - start_time

        # Measure system usage after model execution
        cpu_after = psutil.cpu_percent(interval=None)
        memory_after = psutil.virtual_memory().used / (1024 ** 2)  # Convert to MB

        cpu_usage = cpu_after - cpu_before
        memory_usage = memory_after - memory_before
        gpu_memory_usage, gpu_utilization = get_gpu_usage()

        # Extract content from the Message object
        if response and hasattr(response["message"], "content"):
            response_text = response["message"].content  # Accessing the attribute of the Message object
        else:
            response_text = "No content returned or unexpected format"

        print(f"Prompt: {prompt}\nResponse Time: {latency:.2f} seconds\n")

        # Save the result
        results.append({
            "Prompt": prompt,
            "Response Time (seconds)": latency,
            "Response": response_text,
            "CPU Usage (%)": cpu_usage,
            "Memory Usage (MB)": memory_usage,
            "GPU Memory Usage (MB)": gpu_memory_usage,
            "GPU Utilization (%)": gpu_utilization
        })
    except Exception as e:
        print(f"Error with prompt '{prompt}': {e}")
        results.append({
            "Prompt": prompt,
            "Response Time (seconds)": "Error",
            "Response": f"Error: {str(e)}",
            "CPU Usage (%)": "N/A",
            "Memory Usage (MB)": "N/A",
            "GPU Memory Usage (MB)": "N/A",
            "GPU Utilization (%)": "N/A"
        })20.01.2025

# Save to JSON file
json_file_name = model_name + "_response_time_results_ressours_usage.json"
with open(json_file_name, "w") as file:
    json.dump(results, file, indent=4)

print(f"The results have been saved in {json_file_name}.")

\end{lstlisting}


For this Pyhton Skript we used ChatGPT to help with the psutil library to get the CPU and Memory usage. 
We also used ChatGPT to get more Questions for the data collection. 

The results were saved as JSON files for the later analysis. One Problem we encountered were the tourch librarys, witch 
diden't function probally on the School AI Server, so we couldn't get the GPU usage for the models.

\subsubsection{Used python libaries}

\textbf{psutil libarie}

The \texttt{psutil} library is a Python module that provides an interface for retrieving information on system utilization, 
including CPU, memory, disk, network, and processes. It is commonly used for monitoring and managing system performance and is 
highly efficient due to its low overhead. \texttt{psutil} is cross-platform, supporting major operating systems like Windows, 
Linux, and macOS. It enables developers to create scripts for system diagnostics, process control, and resource management, 
making it an essential tool for performance optimization and system administration in Python-based projects.

\cite{psutil-library-explanation}

\textbf{Ollama}

The \texttt{ollama} library is a Python package designed to provide a seamless interface for interacting with the Ollama application. 
It allows users to easily access and leverage various AI models for natural language processing tasks. 
By simplifying the integration of AI models into Python applications, the library supports a wide range of functionalities, 
making it an efficient tool for developing AI-powered solutions.

\cite{ollama-python-documentation-github}


\subsection{Testresults and Analysis}

Explain how the data was analyzed and interpreted.
implement nice Graphics

\subsection{Model Comparison for different Use Cases and Scenarios}

Explain wich modell is best for what

\subsection{Model Selected for the Final Application}

Explain why the selected model was selected and what the criteria were

and explain the different models.

\subsection{Model Integration and Deployment}

Explain how the model was integrated and deployed

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\section{OpenAI integration with thier API}

We also wanted to Integrade an API wich we can aqiare non open source models. Wich are also running on diverent servers with more powerfull hardware.
Those paid models are also more accurate and faster then the open source models and with the extern computing power we can also run more models at the same time.

We choosed the OpenAI API because it is one of the most popular and best AI API's on the market. It is very well documented and easy to use.
There are many Examples and Tutorials on how to use the API and how to integrate it into your own application.

\subsection{OpenAI API Overview}

\subsection{Data Securety and Privacy}

Write about the Data Securety and Privacy of the OpenAI API
And the conserence that ocurd with the OpenAI API
And write what we did about this (Note for the user on the School Website)

\subsection{OpenAI API implementation in Vue.js}

Write and give a code example on how to implement the OpenAI API in Vue.js
Both JS part and the HTML part











